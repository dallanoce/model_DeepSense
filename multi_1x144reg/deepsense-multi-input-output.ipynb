{"cells":[{"metadata":{"pycharm":{"name":"#%%\n"},"id":"9099zcWyNkoj","trusted":true},"cell_type":"code","source":"\n# Deep Sense","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"rIM7N_IKNkot","outputId":"af30e88a-d203-4fdd-fa40-48590792cabd","trusted":true},"cell_type":"code","source":"import re\nfrom os import listdir\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport datetime\nfrom os.path import isfile, join\nimport random\nimport keras\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, GRUCell, Conv3D, TimeDistributed, Conv1D, Bidirectional, Layer\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, BatchNormalization, GRU\nfrom tensorflow.keras.models import Model, Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, Concatenate\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import Callback\nimport tensorflow.keras.backend as K\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\nimport math\nimport os\nfrom scipy import stats\nfrom sklearn.preprocessing import OneHotEncoder\nimport csv\n\n\n# read in custom modules \n# custom scripts are found\nos.chdir(\"/kaggle/usr/lib\") \nfrom datagenerator_multi_output import DataGenerator\n# reset our working directory\nos.chdir(\"/kaggle/working/\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\ntf.keras.mixed_precision.experimental.set_policy(policy) \n\nprint('Compute dtype: %s' % policy.compute_dtype)\nprint('Variable dtype: %s' % policy.variable_dtype)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Set the seed for random operations.\n# This let our experiments to be reproducible.\nSEED = 1234\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\n\nEPOCH_LENGTH = 30\n\nSAMPLE_RATE = 250\nSAMPLE_RATE_AIRFLOW = 10\nSAMPLE_RATE_BODYPOSITION = 10\n\n\necg_path = '/kaggle/input/shhs-processed/ecgs/shhs2/'\nairflow_path = '/kaggle/input/shhs-processed/airflows/shhs2/'\nhypnogram_path = '/kaggle/input/shhs-processed/hypnogram/shhs2/'\nbp_path = '/kaggle/input/shhs-processed/body_position_encoded/shhs2/'\nheatlhy_path = '/kaggle/input/shhs-processed/'\n\n\nOUT_DIM = 4  # len(idDict)\n\nDATASET_OLD = 'df_all_checkpoint_4.csv'\nDATASET_NEW = 'analysis_all.csv'\n\nBATCH_SIZE = 4\n\nTEST_SIZE = 32\n\nTIME_STEPS = SAMPLE_RATE * EPOCH_LENGTH\nSTEP = 1\nSTAGES = 4\n\n\nclass_weight = {0: 0.7,\n                1: 0.5,\n                2: 0.5,\n                3: 2,\n                4: 1.5,\n               }\n\n\nclass_weight = [0.5,0.5,0.5,1.6,1.2]\n\nclass_weights_status=[1.5,0.7]\n\nlossWeights = {'hypno_output': 1.8, 'status_output': 1.0}\n\n\nFILTER_BEFORE = 48\nFILTER_AFTER = 48\n\nload = False # load pre-trained model\ntb = False #tensorboard\ncp = True # checkpoint\nearly_stop = False # early stoppin","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"6-1UH3CGNkpS","trusted":true},"cell_type":"code","source":"def deepSense_TD(shape_2,shape_3,shape_7,shape_8,shape_9,shape_10):\n\n\n\n    #INDIVIDUAL CONVOLUTION LAYERS\n    \n        #ECH CONV NET\n\n    input_ecg = Input(shape=[None, shape_2,shape_3,1])\n\n    conv1_ecg = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[32,1], activation='relu',strides =(3,1)))(input_ecg)\n    batch1_ecg = TimeDistributed(BatchNormalization())(conv1_ecg)\n\n\n    conv2_ecg = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[32,1], activation='relu',strides =(2,1)))(batch1_ecg)\n    batch2_ecg = TimeDistributed(BatchNormalization())(conv2_ecg)\n\n    conv3_ecg = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[32,1], activation='relu',strides =(2,1)))(batch2_ecg)\n    batch3_ecg = TimeDistributed(BatchNormalization())(conv3_ecg)\n\n    conv4_ecg = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[32,1], activation='relu',strides =(2,1)))(batch3_ecg)\n    batch4_ecg = TimeDistributed(BatchNormalization())(conv4_ecg)\n\n    conv5_ecg = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[16,1], activation='relu',strides =(2,1)))(batch4_ecg)\n    batch5_ecg = TimeDistributed(BatchNormalization())(conv5_ecg)\n\n    conv6_ecg = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[4,1], activation='relu',strides =(1,1)))(batch5_ecg)\n    batch6_ecg = TimeDistributed(BatchNormalization())(conv6_ecg)\n\n    conv7_ecg = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[4,1], activation='relu',strides =(1,1)))(batch6_ecg)\n    batch7_ecg = TimeDistributed(BatchNormalization())(conv7_ecg)\n    \n    conv8_ecg = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[4,1], activation='relu',strides =(1,1)))(batch7_ecg)\n    batch8_ecg = TimeDistributed(BatchNormalization())(conv8_ecg)\n    \n    #flat_ecg = TimeDistributed(Flatten())(batch8_ecg)\n\n    #flat_ecg = Reshape((-1,flat_ecg.shape[2],1,1))(flat_ecg)\n    \n    flat_ecg = Reshape((-1,batch8_ecg.shape[2],batch8_ecg.shape[4],1))(batch8_ecg)\n    \n    \n        #AIRFLOW CONV NET\n        \n    input_air = Input(shape=[None, shape_7,shape_8,1])\n        \n    conv1_air = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[8,1], activation='relu',strides =(2,1)))(input_air)\n    batch1_air = TimeDistributed(BatchNormalization())(conv1_air)\n    \n    conv2_air = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[8,1], activation='relu',strides =(1,1)))(batch1_air)\n    batch2_air = TimeDistributed(BatchNormalization())(conv2_air)\n\n    conv3_air = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[6,1], activation='relu',strides =(1,1)))(batch2_air)\n    batch3_air = TimeDistributed(BatchNormalization())(conv3_air)\n\n    conv4_air = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[4,1], activation='relu',strides =(1,1)))(batch3_air)\n    batch4_air = TimeDistributed(BatchNormalization())(conv4_air)\n    \n    conv5_air = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[4,1], activation='relu',strides =(1,1)))(batch4_air)\n    batch5_air = TimeDistributed(BatchNormalization())(conv5_air)\n    \n    conv6_air = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[4,1], activation='relu',strides =(1,1)))(batch5_air)\n    batch6_air = TimeDistributed(BatchNormalization())(conv6_air)\n    \n    #flat_air = TimeDistributed(Flatten())(batch4_air)\n\n    #flat_air = Reshape((-1,flat_air.shape[2],1,1))(flat_air)\n    \n    flat_air = Reshape((-1,batch6_air.shape[2],batch6_air.shape[4],1))(batch6_air)\n    \n    \n        #BODY POSITION CONV NET\n    \n    \n    input_bp = Input(shape=[None,shape_9,shape_10,1])\n    \n    conv1_bp = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[32,4], activation='relu',strides =(2,1)))(input_bp)\n    batch1_bp = TimeDistributed(BatchNormalization())(conv1_bp)\n    \n    conv2_bp = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[4,1], activation='relu',strides =(1,1)))(batch1_bp)\n    batch2_bp = TimeDistributed(BatchNormalization())(conv2_bp)\n\n    conv3_bp = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[4,1], activation='relu',strides =(1,1)))(batch2_bp)\n    batch3_bp = TimeDistributed(BatchNormalization())(conv3_bp)\n\n    conv4_bp = TimeDistributed(Conv2D(filters=FILTER_BEFORE, kernel_size=[4,1], activation='relu',strides =(1,1)))(batch3_bp)\n    batch4_bp = TimeDistributed(BatchNormalization())(conv4_bp)\n    \n    #flat_bp = TimeDistributed(Flatten())(batch4_bp)\n\n    #flat_bp = Reshape((-1,flat_bp.shape[2],1,1))(flat_bp)\n    \n    flat_bp = Reshape((-1,batch4_bp.shape[2],batch4_bp.shape[4],1))(batch4_bp)\n    \n    \n    \n    #CONCATENATION OF THE 2 NET\n    print(flat_ecg.shape)\n    print(flat_air.shape)\n    print(flat_bp.shape)\n    \n    merge = Concatenate(axis=-2)([flat_ecg,flat_air,flat_bp])\n\n\n    # MERGE CONVOLUTION LAYERS\n    print(merge.shape)\n\n    conv4 = TimeDistributed(Conv2D(filters=FILTER_AFTER, kernel_size=[1,FILTER_BEFORE * 3], activation='relu',strides=(1,1)))(merge)\n    batch4 = TimeDistributed(BatchNormalization())(conv4)\n\n    conv5 = TimeDistributed(Conv2D(filters=FILTER_AFTER, kernel_size=[4,1], activation='relu',strides=(2,1)))(batch4)\n    batch5 = TimeDistributed(BatchNormalization())(conv5)\n\n    conv6 = TimeDistributed(Conv2D(filters=FILTER_AFTER, kernel_size=[8,1], activation='relu',strides=(2,1)))(batch5)\n    batch6 = TimeDistributed(BatchNormalization())(conv6)\n    \n    conv7 = TimeDistributed(Conv2D(filters=FILTER_AFTER, kernel_size=[8,1], activation='relu',strides=(1,1)))(batch6)\n    batch7 = TimeDistributed(BatchNormalization())(conv7)\n    \n    conv8 = TimeDistributed(Conv2D(filters=FILTER_AFTER, kernel_size=[8,1], activation='relu',strides=(1,1)))(batch7)\n    batch8 = TimeDistributed(BatchNormalization())(conv8)\n\n\n    flat8 = TimeDistributed(Flatten())(batch8)\n\n\n    #RECURRENT LAYERS FOR HYPNOGRAM\n\n    gru1 = GRU(256, activation='relu',return_sequences=True, kernel_regularizer=l2(0.01))(flat8)\n    drop1 = Dropout(rate=0.4)(gru1)\n    batch1 = BatchNormalization()(drop1)\n\n    gru2 = GRU(128, activation='relu',return_sequences=True, kernel_regularizer=l2(0.01))(batch1)\n    drop2 = Dropout(rate=0.4)(gru2)\n    batch2 = BatchNormalization()(drop2)\n\n\n    dense_hypno = TimeDistributed(Dense(OUT_DIM + 1, activation='softmax',dtype=tf.float32),name = 'hypno_output')(batch2)\n    \n    \n    # RECURRENT LAYER FOR HEALTHY - UNHEALTHY\n    \n    gru3 = GRU(256, activation='relu',return_sequences=True, kernel_regularizer=l2(0.01))(flat8)\n    drop3 = Dropout(rate=0.4)(gru3)\n    batch3 = BatchNormalization()(drop3)\n\n    gru4 = GRU(128, activation='relu',return_sequences=False, kernel_regularizer=l2(0.01))(batch3)\n    drop4 = Dropout(rate=0.4)(gru4)\n    batch4 = BatchNormalization()(drop4)\n    \n    dense_healthy = Dense(1, activation='sigmoid',name = 'status_output',dtype=tf.float32)(batch4)\n    \n    \n    return [input_ecg,input_air,input_bp], [dense_hypno, dense_healthy]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n\n#F1 Metrics\ndef f1(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"bkVCbUHUNkpW","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"Bg_WUsPVNkpZ","outputId":"5094db07-2cb5-4bab-f8ad-1e1c9611b138","trusted":true},"cell_type":"code","source":"\nrandom.seed(SEED)\n\nif not load:\n    onlyfiles = [f for f in listdir(ecg_path) if isfile(join(ecg_path, f))]\n    \n    id = [re.search('(.+?).npz', x).group(1) for x in onlyfiles]\n    id.sort()\n    \n    #print(id)\n    \n    print(len(id))\n    \n    np.random.seed(SEED)\n    id_test = np.random.choice(id, size=TEST_SIZE,replace=False)\n    \n    id = list(set(id) - set(id_test))\n    \n    \n    id_validation = np.random.choice(id, size=TEST_SIZE,replace=False)\n    \n    id = list(set(id) - set(id_validation))\n    \n    #id = id[0:int(len(id)*0.2)]\n    \n    print(len(id))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"3qW-Lz24Nkpf","trusted":true},"cell_type":"code","source":"# learning rate\nlr = 1*1e-4\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#custom loss \nfrom sklearn.metrics import hamming_loss\n\n\ndef hamming(y_true, y_pred):\n    \n    \n    return hamming_loss_fn(y_true,y_pred)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nBATCH_SIZE = 2*tpu_strategy.num_replicas_in_sync\nwith tpu_strategy.scope():\n    input, output = deepSense_TD(SAMPLE_RATE*EPOCH_LENGTH,1,SAMPLE_RATE_AIRFLOW*EPOCH_LENGTH,1)\n    model = Model(inputs=input,outputs=output)\n    optimizer = keras.optimizers.Adam(learning_rate=lr)\n    # Compile Model\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy',\n        metrics=['accuracy'])\n    \ntraining_generator = DataGenerator(id,  ecg_path=ecg_path, airflow_path=airflow_path,hypnogram_path=hypnogram_path, batch_size=BATCH_SIZE)\nvalidation_generator = DataGenerator(id_validation,  ecg_path=ecg_path, airflow_path=airflow_path,hypnogram_path=hypnogram_path, batch_size=BATCH_SIZE)\n  \"\"\"  \n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"input, output = deepSense_TD(SAMPLE_RATE*EPOCH_LENGTH,1,SAMPLE_RATE_AIRFLOW*EPOCH_LENGTH,1,SAMPLE_RATE_BODYPOSITION*EPOCH_LENGTH,4)\nmodel = Model(inputs=input,outputs=output)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n#optimizer = tf.train.experimental.enable_mixed_precision_graph_rewrite(optimizer, loss_scale='dynamic')\n\n\n# Compile Model\nmodel.compile(optimizer=optimizer, loss={\n                  'hypno_output': 'sparse_categorical_crossentropy', \n                  'status_output': 'binary_crossentropy'},\n              loss_weights=lossWeights,\n              metrics={\n                  'hypno_output': 'sparse_categorical_accuracy', \n                  'status_output': 'binary_accuracy'},\n              sample_weight_mode='temporal')\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load pre-trained model\nload_from = \"/kaggle/input/model-deepsense/multi_1x144/\"\n\nif load:\n    \n    model = load_model(load_from + 'cp_42.h5')\n    \n    onlyfiles = [f for f in listdir(ecg_path) if isfile(join(ecg_path, f))]\n\n    id = [re.search('(.+?).npz', x).group(1) for x in onlyfiles]\n    id.sort()\n    \n    print(len(id))\n    \n    id_test = np.loadtxt(load_from + 'id_test.txt',dtype='str')\n    id_validation = np.loadtxt(load_from + 'id_validation.txt',dtype='str')\n    \n    print(id_validation)\n\n    id = list(set(id) - set(id_test))\n    id = list(set(id) - set(id_validation))\n    \n    \n    \n    print(len(id))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Saving id of testing and validation sample","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('id_validation.txt', 'w') as f:\n    for item in id_validation:\n        f.write(\"%s\\n\" % item)\nprint(id_validation)\n\nwith open('id_test.txt', 'w') as f:\n    for item in id_test:\n        f.write(\"%s\\n\" % item)\nprint(id_test)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"nD-Tq9O5Nkpn","outputId":"444d3860-c20b-4f6c-89e9-535ade7557e5","trusted":true},"cell_type":"code","source":"training_generator = DataGenerator(id,  ecg_path=ecg_path, airflow_path=airflow_path,bp_path=bp_path, \n                                   hypnogram_path=hypnogram_path,healthy_path=heatlhy_path, batch_size=BATCH_SIZE, \n                                   class_weights = class_weight,class_weights_status=class_weights_status,  weights= True)\n\nvalidation_generator = DataGenerator(id_validation,  ecg_path=ecg_path, airflow_path=airflow_path, bp_path=bp_path, \n                                     hypnogram_path=hypnogram_path, healthy_path=heatlhy_path, batch_size=BATCH_SIZE, \n                                     class_weights = class_weight,class_weights_status=class_weights_status,  weights= True)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"KrKiFcGxNkps","trusted":true},"cell_type":"code","source":"\n\nmodel._layers = [\n    layer for layer in model._layers if isinstance(layer, Layer)\n]\n\ntf.keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"id":"NdT8rl44Nkp0","trusted":true},"cell_type":"code","source":"callbacks = []\ntf.random.set_seed(SEED)\n\n# Model checkpoint\n\n\nif cp:\n\n    ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath='cp_{epoch:02d}.h5', \n                                                   save_weights_only=False)  # False to save the model directly\n    callbacks.append(ckpt_callback)\n\n\n# Configure the TensorBoard callback and fit your model\n\nif tb:\n    tensorboard_callback = keras.callbacks.TensorBoard(\"logs\", profile_batch=0)\n    callbacks.append(tensorboard_callback)\n\n# Early Stopping\n\nif early_stop: #using early stopping on validation accuracy\n    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=5, restore_best_weights=True )\n    callbacks.append(es_callback)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n","is_executing":true},"id":"BtZixzb7Nkp4","outputId":"4a6ae49e-3278-4018-a019-4fe2411deda7","trusted":true},"cell_type":"code","source":"\n\nhistory = model.fit(training_generator, \n                    validation_data=validation_generator, \n                    epochs=4,\n                    use_multiprocessing=True,\n                    #max_queue_size = 1,\n                    #class_weight=class_weight,\n                    callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\n\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy',color='w') \nplt.ylabel('accuracy',color='w')\nplt.xlabel('epoch',color='w')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss',color='w')\nplt.ylabel('loss',color='w')\nplt.xlabel('epoch',color='w')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the model\nfrom datetime import date \n\ntoday = date.today()\n\n\nmodel.save('my_model_' +'.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python3 -c 'import tensorflow as tf; print(tf.__version__)'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict(validation_generator,steps=1)\nprint(prediction)\nprediction = np.argmax(prediction,axis=2)\nprint(prediction)\nfirst = prediction[0]\nprint(first[1000:1050])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dg = DataGenerator(['204565','201048', '200145', '204285'],  ecg_path=ecg_path, airflow_path=airflow_path, bp_path=bp_path, hypnogram_path=hypnogram_path, batch_size=BATCH_SIZE,shuffle=False)\n\nprediction = model.predict(dg)\n#print(prediction)\nprediction = np.argmax(prediction,axis=2)\n#print(prediction)\nfirst = prediction[0]\nprint(first[500:550])\nprint(np.count_nonzero(first==0))\nprint(np.count_nonzero(first==1))\nprint(np.count_nonzero(first==2))\nprint(np.count_nonzero(first==3))\nprint(np.count_nonzero(first==4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hypnogram = pd.read_csv(hypnogram_path + '204565' + '.csv', usecols=['Stage'])\nhypnogram.rename(columns={'Sleep': 'Y'}, inplace=True)\n        \nhypnogram_reshaped = np.array(hypnogram).reshape(-1, 1)\n\nprint(hypnogram_reshaped[500:550])\n\nprint(np.count_nonzero(hypnogram_reshaped==0))\nprint(np.count_nonzero(hypnogram_reshaped==1))\nprint(np.count_nonzero(hypnogram_reshaped==2))\nprint(np.count_nonzero(hypnogram_reshaped==3))\nprint(np.count_nonzero(hypnogram_reshaped==4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_generator = DataGenerator(id_test,  ecg_path=ecg_path, airflow_path=airflow_path, bp_path=bp_path, hypnogram_path=hypnogram_path, batch_size=BATCH_SIZE)\nmodel.evaluate(training_generator,steps=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tf-nightly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python3 -c 'import tensorflow as tf; print(tf.__version__)'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'id_test' +'.txt')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}